"""
LLM Q&A Workflow using LangGraph

This script demonstrates how to integrate a language model (LLM) into a LangGraph workflow
to create a simple question-answering system.

Key concepts:
- Integrating OpenAI's ChatModel with LangGraph
- Handling environment variables securely
- Using LLM for generating responses in a workflow
"""

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from typing import TypedDict
from dotenv import load_dotenv
import os

# Load environment variables from .env file (API keys, etc.)
load_dotenv(override=True)

# Validate API key configuration
api_key = os.environ.get("OPENAI_API_KEY")
if api_key:
    print(f"API key found! First 5 chars: {api_key[:5]}...")
    print(f"API key length: {len(api_key)}")
    # Print the key source if possible
    try:
        from dotenv import find_dotenv
        env_path = find_dotenv()
        print(f"Environment file path: {env_path}")
    except:
        print("Could not determine environment file path")
else:
    print("No API key found in environment variables!")
    print("Please ensure you have an OPENAI_API_KEY in your .env file")
    exit(1)  # Exit if no API key is found

# Initialize the language model
model = ChatOpenAI(openai_api_key=api_key)

# Define the state structure for our Q&A workflow
class LLMState(TypedDict):
    """State definition for LLM Q&A workflow.
    
    Attributes:
        question: The input question from the user
        answer: The answer generated by the language model
    """
    question: str
    answer: str

def llm_qa(state: LLMState) -> LLMState:
    """Generate an answer to the given question using the LLM.
    
    Args:
        state: Current workflow state containing the question
        
    Returns:
        Updated state with the generated answer
    """
    # Extract the question from state
    question = state['question']

    # Form a prompt for the LLM
    prompt = f'Answer the following question: {question}'

    # Ask the question to the LLM
    answer = model.invoke(prompt).content

    # Update the answer in the state
    state['answer'] = answer

    print(f"Question: {state['question']}, Answer: {state['answer']}")

    return state

# Create the workflow graph
graph = StateGraph(LLMState)

# Add the LLM Q&A node to the graph
graph.add_node('llm_qa', llm_qa)

# Define the flow: START -> llm_qa -> END
graph.add_edge(START, 'llm_qa')
graph.add_edge('llm_qa', END)

# Compile the workflow
workflow = graph.compile()

# Execute with an initial question
initial_state = {'question': 'How far is moon from the earth?'}
final_state = workflow.invoke(initial_state)

# Print the answer
print(final_state['answer'])